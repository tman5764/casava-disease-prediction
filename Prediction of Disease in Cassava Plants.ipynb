{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Prediction of Disease in Cassava Plants.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1P8XWctI7_n9DSrsccIiJHtCY7cK5fgRZ","authorship_tag":"ABX9TyMcyIPT51ZSrYgjJt7v9WuM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"Ci6htg-aYZay","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627442854995,"user_tz":300,"elapsed":1668,"user":{"displayName":"Tyler Gardner","photoUrl":"","userId":"07152079335319340371"}},"outputId":"e2e6c72b-d5a9-44c4-8819-07854accf30f"},"source":["import tensorflow as tf\n","print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Num GPUs Available:  1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nykDVPtSfWMY"},"source":["# Prediction of Disease in Casava Plants"]},{"cell_type":"markdown","metadata":{"id":"tdBY3jGffZyk"},"source":["# Loading of needed Librarys and data"]},{"cell_type":"code","metadata":{"id":"3orsG_TAfmH7","executionInfo":{"status":"ok","timestamp":1627442900053,"user_tz":300,"elapsed":837,"user":{"displayName":"Tyler Gardner","photoUrl":"","userId":"07152079335319340371"}}},"source":["#Load Nessicery Librarys\n","import numpy as np \n","import pandas as pd \n","\n","import cv2\n","import os\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn import metrics\n","# Any results you write to the current direct\n","\n","\n","\n","#ory are saved as output.\n","\n","from time import time\n","\n","from sklearn.model_selection import train_test_split\n","\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.applications.vgg16 import VGG16\n","\n","from tensorflow.keras.callbacks import TensorBoard\n","\n","import keras\n","from keras.models import Sequential\n","from keras.models import model_from_json\n","from keras.layers.normalization import BatchNormalization\n","from keras.layers.convolutional import Conv2D\n","from keras.layers.convolutional import MaxPooling2D\n","from keras.layers.core import Activation\n","from keras.layers.core import Flatten\n","from keras.layers.core import Dropout\n","from keras.layers.core import Dense\n","from keras import backend as K\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n","\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import plot_confusion_matrix\n","\n","import math"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"T9_zlpeyf0Dq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627442902861,"user_tz":300,"elapsed":15,"user":{"displayName":"Tyler Gardner","photoUrl":"","userId":"07152079335319340371"}},"outputId":"51166ddd-8dc6-4da6-f027-09b7656cd6c4"},"source":["train = pd.read_csv(\"C:/Users/twg57/Downloads/Cassava-20210716T024150Z-001/Cassava/train.csv\",dtype=str)\n","train_path = \"C:/Users/twg57/Downloads/Cassava-20210716T024150Z-001/Cassava/train_images\"\n","test = pd.read_csv(\"C:/Users/twg57/Downloads/Cassava-20210716T024150Z-001/Cassava/sample_submission.csv\",dtype=str)\n","test_path = \"C:/Users/twg57/Downloads/Cassava-20210716T024150Z-001/Cassava/test_images\"\n","print(train.head())"],"execution_count":3,"outputs":[{"output_type":"stream","text":["         image_id label\n","0  1000015157.jpg     0\n","1  1000201771.jpg     3\n","2   100042118.jpg     1\n","3  1000723321.jpg     1\n","4  1000812911.jpg     3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Od78weiVk7N_"},"source":["# Exploatory analysis"]},{"cell_type":"markdown","metadata":{"id":"9OTe9N7JlBmo"},"source":["# Label Exploration\n"]},{"cell_type":"code","metadata":{"id":"Q5uFJpkWgud_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627441975443,"user_tz":300,"elapsed":15,"user":{"displayName":"Tyler Gardner","photoUrl":"","userId":"07152079335319340371"}},"outputId":"d5e1a1fe-7f4c-402e-f977-f18877a76a7a"},"source":["print('Training set summary Stats')\n","print('Number of image : ', len(train))\n","print('Number of images per catagory : ', train['label'].value_counts())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training set summary Stats\n","Number of image :  21397\n","Number of images per catagory :  3    13158\n","4     2577\n","2     2386\n","1     2189\n","0     1087\n","Name: label, dtype: int64\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LDolzfxDhjN1","colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"status":"ok","timestamp":1626405098843,"user_tz":300,"elapsed":171,"user":{"displayName":"Tyler Gardner","photoUrl":"","userId":"07152079335319340371"}},"outputId":"35c36ab5-dfe3-4c33-ca83-dc5d59948f26"},"source":["df = train['label'].value_counts()\n","df = pd.DataFrame(df)\n","df.plot.pie(y='label')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<AxesSubplot:ylabel='label'>"]},"metadata":{"tags":[]},"execution_count":6},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"lK6QIZa2lckf"},"source":["Form this it appears that the data is not balanced. There are far more amount of the 3rd response or, Cassava Mosaic Disease (CMD). This could lead to some overfitting, but we will im sure when we run the model we can account for that. "]},{"cell_type":"markdown","metadata":{"id":"baSrVCD4l3ve"},"source":["# Image analysis"]},{"cell_type":"code","metadata":{"id":"1qXVNdeqi5fQ","colab":{"base_uri":"https://localhost:8080/","height":520},"executionInfo":{"status":"error","timestamp":1626405122217,"user_tz":300,"elapsed":110,"user":{"displayName":"Tyler Gardner","photoUrl":"","userId":"07152079335319340371"}},"outputId":"763613df-0ccb-4f70-fc4c-b6b434aee268"},"source":["def plot_imgs(img_ids, labels):\n","    plt.figure(figsize=(20, 18))\n","    \n","    num_samples = len(labels)\n","    for idx, (img_id, label) in enumerate(zip(img_ids, labels)):\n","        num_cols = 3\n","        num_rows = (num_samples // num_cols) + (num_samples % num_cols)\n","        plt.subplot(num_rows, num_cols, idx + 1)\n","        \n","        img = cv2.imread(train_path + '/' + img_id)\n","        \n","        # Reversing the color channle from BGR to RGB\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","        plt.imshow(img)\n","        plt.title(name_of_diseases[int(label)], fontsize=18)\n","        plt.axis('off')\n","        \n","    plt.show()\n","    \n","name_of_diseases = pd.read_json('C:/Users/twg57/Downloads/Cassava-20210716T024150Z-001/Cassava/label_num_to_disease_map.json', typ='series')   \n","tmp_df  = train.sample(15)\n","img_ids = tmp_df.image_id.values\n","labels  = tmp_df.label.values\n","plot_imgs(img_ids, labels)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"error","evalue":"ignored","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[1;32m<ipython-input-7-1c118f8b61b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mimg_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mlabels\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtmp_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mplot_imgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32m<ipython-input-7-1c118f8b61b0>\u001b[0m in \u001b[0;36mplot_imgs\u001b[1;34m(img_ids, labels)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m# Reversing the color channle from BGR to RGB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31merror\u001b[0m: OpenCV(4.0.1) C:\\ci\\opencv-suite_1573470242804\\work\\modules\\imgproc\\src\\color.cpp:181: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1440x1296 with 1 Axes>"],"image/png":"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\n"},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"-3riAC9gsjSj"},"source":["The pictues are of the leaves of the plant. From looking at othes that are in the data set there are some of the stalks, as well as roots. Also the pictures range in level of closeness to the plant, as well as the lighting. "]},{"cell_type":"markdown","metadata":{"id":"8TFQCRCLxf4u"},"source":["# Image Generator "]},{"cell_type":"markdown","metadata":{"id":"Z17KHs0Lxxap"},"source":["In this section we will be creating the image generator's. These are needed to make sure that the image data can be read into the CNN model, inorder to analyze the images so that we can create the model to predict off new images. "]},{"cell_type":"code","metadata":{"id":"fjSsnAOpxxI7","colab":{"base_uri":"https://localhost:8080/","height":588},"executionInfo":{"status":"error","timestamp":1626405143520,"user_tz":300,"elapsed":25,"user":{"displayName":"Tyler Gardner","photoUrl":"","userId":"07152079335319340371"}},"outputId":"3acebd0b-94dd-40c3-d7b6-b9db247702fe"},"source":["#Determin the image size\n","img = plt.imread(\"/content/drive/MyDrive/Kaggle Projects/Cassava/train_images/\"+train.iloc[0]['image_id'])\n","print('Images shape', img.shape)\n","\n","img_test = plt.imread(\"/content/drive/MyDrive/Kaggle Projects/Cassava/test_images/\"+test.iloc[0]['image_id'])\n","print('Images shape', img_test.shape)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m<ipython-input-8-71df8962251c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Determin the image size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/content/drive/MyDrive/Kaggle Projects/Cassava/train_images/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'image_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Images shape'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mimg_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/content/drive/MyDrive/Kaggle Projects/Cassava/test_images/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'image_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mimread\u001b[1;34m(fname, format)\u001b[0m\n\u001b[0;32m   2244\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2245\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2246\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mimread\u001b[1;34m(fname, format)\u001b[0m\n\u001b[0;32m   1494\u001b[0m                     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1495\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1496\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mimg_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1497\u001b[0m         return (_pil_png_to_float_array(image)\n\u001b[0;32m   1498\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPIL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPngImagePlugin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPngImageFile\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   2910\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2911\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2912\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2913\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Kaggle Projects/Cassava/train_images/1000015157.jpg'"]}]},{"cell_type":"markdown","metadata":{"id":"jn1WceZOzZDU"},"source":["The images are 600 by 800 pixels. Due to the size of the images, we will need to resize them. \n","\n","Initially i resized the images to 94 by 94. THe model majorly overfit and did not learn a lot form the images. This was due to the images being so compressed that they not a lot could be gathered form the images. \n","\n","The second size that i resized the image to was 300 by 300. This did provide a much better model, but it was still not getting as good of results that i was hoping for. \n","\n","Finally I resized it to 150 by 200. This was the best image size and it wored really well for the model. "]},{"cell_type":"code","metadata":{"id":"8LyIGyi-qBNq","executionInfo":{"status":"ok","timestamp":1627442912204,"user_tz":300,"elapsed":9,"user":{"displayName":"Tyler Gardner","photoUrl":"","userId":"07152079335319340371"}}},"source":["train_datagen = ImageDataGenerator(rescale=1/255, validation_split=0.25,\n","                                  rotation_range=10, # rotation\n","                                  width_shift_range=0.2, # horizontal shift\n","                                  height_shift_range=0.2, # vertical shift\n","                                  zoom_range=0.2) # zoom\n","Val_datagen = ImageDataGenerator(rescale=1/255, validation_split=0.25)\n","test_datagen = ImageDataGenerator(rescale=1/255)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xw2w5-1w2KQR"},"source":["This indicates how the data generator will operate. For the training data initially i was not including random shifts in the data. When i did not include those the model massivly overfit. It was not learning enough of the usfull information of the pictures and more form the noise in the pictures. Once in included the changes to the images which were rotations, height and weidth changes and zooms into the images the model started to prefrome much better."]},{"cell_type":"code","metadata":{"id":"46hANNAS3VVF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627442915910,"user_tz":300,"elapsed":1663,"user":{"displayName":"Tyler Gardner","photoUrl":"","userId":"07152079335319340371"}},"outputId":"0a4f387a-dc36-482f-9d57-ba5baf82a86d"},"source":["bs = 256\n","\n","train_generator = train_datagen.flow_from_dataframe(\n","    dataframe = train,\n","    directory = train_path,\n","    x_col = \"image_id\",\n","    y_col = \"label\",\n","    subset = \"training\",\n","    batch_size = bs,\n","    #seed = 1,\n","    shuffle = True,\n","    class_mode = \"categorical\",\n","    target_size = (150,200))\n","\n","valid_generator = Val_datagen.flow_from_dataframe(\n","    dataframe = train,\n","    directory = train_path,\n","    x_col = \"image_id\",\n","    y_col = \"label\",\n","    subset = \"validation\",\n","    batch_size = bs,\n","    #seed = 1,\n","    shuffle = True,\n","    class_mode = \"categorical\",\n","    target_size = (150,200))\n","\n","test_generator = test_datagen.flow_from_dataframe(\n","    dataframe = test,\n","    directory = test_path,\n","    x_col = \"image_id\",\n","    y_col = None,\n","    batch_size = bs,\n","    seed = 1,\n","    shuffle = False,\n","    class_mode = None,\n","    target_size = (150,200))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["C:\\Users\\twg57\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\dataframe_iterator.py:279: UserWarning: Found 3520 invalid image filename(s) in x_col=\"image_id\". These filename(s) will be ignored.\n","  warnings.warn(\n"],"name":"stderr"},{"output_type":"stream","text":["Found 13408 validated image filenames belonging to 5 classes.\n","Found 4469 validated image filenames belonging to 5 classes.\n","Found 1 validated image filenames.\n"],"name":"stdout"},{"output_type":"stream","text":["C:\\Users\\twg57\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\dataframe_iterator.py:279: UserWarning: Found 3520 invalid image filename(s) in x_col=\"image_id\". These filename(s) will be ignored.\n","  warnings.warn(\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"EKdqsSQu3e6K","executionInfo":{"status":"ok","timestamp":1627442918062,"user_tz":300,"elapsed":2,"user":{"displayName":"Tyler Gardner","photoUrl":"","userId":"07152079335319340371"}}},"source":["tr_size = 16048\n","va_size = 5349\n","te_size = 1\n","tr_steps = math.ceil(tr_size / bs)\n","va_steps = math.ceil(va_size / bs)\n","te_steps = math.ceil(te_size / bs)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"98X54BfT34I7"},"source":["This is what is calculateing the batch size for the model. These are essentiually the amout of images that will be loaded in at a time, so that there are only a few images in there as opposed to all of the images. "]},{"cell_type":"markdown","metadata":{"id":"kNFwUIEy4SvV"},"source":["# CNN Model"]},{"cell_type":"code","metadata":{"id":"oqdbJjQu33fT"},"source":["np.random.seed(1)\n","\n","ConVo_model = Sequential()\n","ConVo_model.add(Conv2D(filters = 64, kernel_size = (5,5), padding = 'same', activation = 'relu', input_shape = (150,200, 3)))\n","ConVo_model.add(Conv2D(filters = 64, kernel_size = (5,5), padding = 'same', activation = 'relu'))\n","ConVo_model.add(MaxPooling2D(pool_size = (2,2)))\n","ConVo_model.add(BatchNormalization())\n","ConVo_model.add(Dropout(0.25))\n","ConVo_model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = 'same', activation = 'relu'))\n","ConVo_model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = 'same', activation = 'relu'))\n","ConVo_model.add(MaxPooling2D(pool_size = (2,2)))\n","ConVo_model.add(BatchNormalization())\n","ConVo_model.add(Dropout(0.5))\n","ConVo_model.add(Conv2D(filters = 128, kernel_size = (2,2), padding = 'same', activation = 'relu'))\n","ConVo_model.add(Conv2D(filters = 128, kernel_size = (2,2), padding = 'same', activation = 'relu'))\n","ConVo_model.add(MaxPooling2D(pool_size = (2,2)))\n","ConVo_model.add(BatchNormalization())\n","ConVo_model.add(Dropout(0.5))\n","ConVo_model.add(Flatten())\n","ConVo_model.add(Dense(128, activation='relu'))\n","ConVo_model.add(Dropout(0.75))\n","ConVo_model.add(BatchNormalization())\n","ConVo_model.add(Dense(5, activation='softmax'))\n","\n","ConVo_model.summary()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CCYvR-Pb8I_i"},"source":["This is the last model that was created for this project. lets talk about a few of the ones that lead us to this one.\n","\n","The first model was really just a proof of concecept, to make sure that the code that i had written worked. It overfit really badly. the training set resulted in 90% accuracy, and the training was at around 60%.\n","\n","The next few models that were created were larger amounts of epochs that were run. the images in these were also larger, as the first model had images that were 94 by 94 pixels. these were 300 x 300. in this case the model wasnt overfitting as bad but the model wasnt learning anything. IT was sitting at around 60%'s to the low 70%'s for accuracy. This may also of been due to the large amount of layes that were in the model. at one point there were aroiund 12 convolutionil layers, which were causing the model to overfit more and not learn to much off the noise in the images.\n","\n","To combat that i took out some of the convolution layers, and added the image alterations to the training images. These really helped to make sure that the model was learinig more and not overfitting. From there i started makeing small changes till i got to the model that i have. This model starts off in the first official layers, haveing 64 filters that are 5x5. after the first initial set of convolution layers, the next couple sets have 128 filters, which went down in size to 3x3 and then 2x2. Also i made sure that there were dropout layers, between each set of convolution layers, and increasing the dropout percentage the deeper the model goes, to help reduce the amount of overfitting. "]},{"cell_type":"code","metadata":{"id":"xIvSuJJJ7VcS"},"source":["%%time \n","\n","opt = keras.optimizers.Adam(0.01)\n","ConVo_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n","\n","h1 = ConVo_model.fit(train_generator, steps_per_epoch=tr_steps, epochs=70,\n","                       validation_data=valid_generator, validation_steps=va_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r9ZUVZKME-hg"},"source":["You will want a GPU when you run this section\n","\n","The model was originally run for 10 epochs, then 20, and finally i realized that 70 epochs provided the best preformance. This model scored around 79% on Kaggle. "]},{"cell_type":"code","metadata":{"id":"IhYguKkA9iKl"},"source":["#Plots the training and validation accuracy and loss\n","\n","start = 1\n","ep_rng = np.arange(start,len(h1.history['accuracy']))\n","\n","plt.figure(figsize=[12,6])\n","plt.subplot(1,2,1)\n","plt.plot(ep_rng, h1.history['accuracy'][start:], label='Training Accuracy')\n","plt.plot(ep_rng, h1.history['val_accuracy'][start:], label='Validation Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend()\n","\n","plt.subplot(1,2,2)\n","plt.plot(ep_rng, h1.history['loss'][start:], label='Training Loss')\n","plt.plot(ep_rng, h1.history['val_loss'][start:], label='Validation Loss')\n","plt.xlabel('Epoch')\n","plt.legend()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0UoTiks49oM5"},"source":["# Save model Weights\n","## See next file for the submission file"]},{"cell_type":"code","metadata":{"id":"nR1vW2BY96SA"},"source":["#Save Model Weights and Configuration\n","ConVo_model.save('models/my_model_v06.h5')"],"execution_count":null,"outputs":[]}]}